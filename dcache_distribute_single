#!/usr/bin/env python
# Tool to distribute a single datasets on dCache
# (tries to accomplish an optimal distribution via simulated annealing)
# by Fred Stober (stober@cern.ch)

import sys, os, copy, random, time, threading, optparse, json
from utils import get_cached, expath, progress
from toolKIT.chimera import dCacheInfo, get_chimera_data
from toolKIT.dCache import get_pool_infos
from toolKIT.lfntools import fn2ddn
from toolKIT.dCache_distribute import filterMoveable, writeTransferCommands

dCacheWebHost = 'http://192.108.45.36:2288'
chimera_path = expath('~/storage_scratch/chimera-dump/current')

parser = optparse.OptionParser()
parser.add_option("-f", "--filename", dest="fn", default=None, help="select by filename")
parser.add_option("-m", "--maxfiles", dest="maxfiles", default=None, type=int, help="select by max number of files in dataset")
parser.add_option("-l", "--filelist", dest="filelist", default=None, help="file with fileinfos")
parser.add_option("-S", "--store_filelist", dest="store_filelist", default=None, help="store fileinfos")
parser.add_option("-L", "--load_transfers", dest="load_transfers", default=None, help="store transfers")

parser.add_option("-t", "--threads", dest="Nthreads", default=10, type=int, help="number of threads")
parser.add_option("-i", "--iterations", dest="Niter", default=100, type=int, help="number of iterations")
parser.add_option("-s", "--max_steps", dest="Nsteps", default=36, type=int, help="number of steps")
parser.add_option("-T", "--store_transfers", dest="store_transfers", default='transfers', help="store transfers")
opts, args = parser.parse_args()

# Select files according to lfn / number of files in dataset
def getFileInfos(opts):
	if opts.filelist:
		return json.load(open(opts.filelist))
	fileInfos = []
	ds_files = {}
	if opts.maxfiles:
		for entry in progress(filterMoveable(get_chimera_data(chimera_path))):
			ds = fn2ddn(entry[dCacheInfo.pfn])
			ds_files[ds] = ds_files.get(ds, 0) + 1
		print len(ds_files), len(filter(lambda ds: ds_files.get(ds, 0) <= opts.maxfiles, ds_files))
	for entry in progress(filterMoveable(get_chimera_data(chimera_path))):
		if opts.fn and (opts.fn not in entry[dCacheInfo.pfn]):
			continue
		if opts.maxfiles:
			ds = fn2ddn(entry[dCacheInfo.pfn])
			if ds_files.get(ds, 0) > opts.maxfiles:
				continue
		if 'disk-only' not in entry[dCacheInfo.pfn]:
			continue
		for loc in entry[dCacheInfo.location]:
			fileInfos.append((entry[dCacheInfo.dcache_id], loc, entry[dCacheInfo.size]))
	if opts.store_filelist:
		json.dump(fileInfos, open(opts.store_filelist, 'w'))
	return fileInfos

# Apply transfer list to fileinfos
def applyTransfers(fileList, transferList):
	idlocMap = dict(map(lambda (id, size, s, t): (id + s, t), transferList))
	return map(lambda (id, loc, size): (id, idlocMap.get(id + loc, loc), size), fileList)

# Get distribution by pool from filelist
def getPoolOverview(fi):
	myFiles = {}
	mySizes = {}
	idlocset = set()
	for fidx, fdata in enumerate(fi):
		(id, loc, size) = fdata
		idloc = id + loc
		if idloc in idlocset:
			return None
		idlocset.add(idloc)
		myFiles[loc] = myFiles.get(loc, 0) + 1
		mySizes[loc] = mySizes.get(loc, 0) + size
	return (myFiles, mySizes, idlocset)

# Optimize filelist to reach goals
def optimizeFileInfos_annealing(fileInfos, goalFiles, goalSizes):
	totalSize = sum(goalSizes.values())
	pool_list = goalSizes.keys()

	# Calculate metric - taking changes into account
	def getMetric(fi, changes, curFiles, curSizes, curIDLocSet):
		myFiles = dict(curFiles)
		mySizes = dict(curSizes)
		myIDLocSet = set(curIDLocSet)
		for idx in changes:
			(id, loc_old, size) = fi[idx]
			loc_new = changes[idx]
			if loc_old == loc_new:
				continue
			idloc_old = id + loc_old
			idloc_new = id + loc_new
			if idloc_new in myIDLocSet:
				return 1e10
			myIDLocSet.remove(idloc_old)
			myIDLocSet.add(idloc_new)
			myFiles[loc_old] = myFiles.get(loc_old, 0) - 1
			mySizes[loc_old] = mySizes.get(loc_old, 0) - size
			myFiles[loc_new] = myFiles.get(loc_new, 0) + 1
			mySizes[loc_new] = mySizes.get(loc_new, 0) + size

		result = 0
		for pool in myFiles:
			result += ((myFiles.get(pool, 0) - goalFiles.get(pool, 0)) / float(len(fileInfos)))**2
			result += ((mySizes.get(pool, 0) - goalSizes.get(pool, 0)) / float(totalSize))**2
		return result

	# Minizize metric
	fileInfos_old = list(fileInfos)
	curFiles, curSizes, curIDLocSet = getPoolOverview(fileInfos)
	m_start = getMetric(fileInfos, {}, curFiles, curSizes, curIDLocSet)
	print 'Start metric', m_start
	m_best = m_start

	N = opts.Niter
	r_prev = 0
	c_max = float(opts.Nsteps * N)
	c_cur = 0
	t_est = 0
	t_start = time.time()
	for step in range(opts.Nsteps, 0, -1):
		for iter in range(N):
			c_cur += 1
			if c_cur % 100 == 0:
				iter_per_sec = c_cur / (time.time() - t_start)
				t_est = (c_max - c_cur) / iter_per_sec
			sys.stdout.write('Progress: %4d %4d %3d%% %5ds ' % (step, iter, 100. * c_cur / c_max, t_est))
			sys.stdout.write('%5.2fx %5.2f %%\r' % (m_start / m_best, r_prev))
			sys.stdout.flush()
			def shuffleFiles(fi, results):
				changes = {}
				for move in range(step):
					idx = random.randint(0, len(fi) - 1)
					(id, loc, size) = fileInfos[idx]
					changes[idx] = pool_list[random.randint(0, len(pool_list) - 1)]
				m_cur = getMetric(fi, changes, curFiles, curSizes, curIDLocSet)
				results[m_cur] = changes
			results = {}
			threads = []
			shuffleFiles(fileInfos, results)
#			for t in range(opts.Nthreads):
#				t = threading.Thread(target = shuffleFiles, args = (fileInfos, results))
#				t.start()
#				threads.append(t)
#			for t in threads:
#				t.join()
			m_cur = min(results)
			if m_cur < m_best:
				r_prev = (m_best / m_cur - 1) * 100
				m_best = m_cur
				best_change = results[m_cur]
				for idx in best_change:
					(id, loc, size) = fileInfos[idx]
					fileInfos[idx] = (id, best_change[idx], size)
				curFiles, curSizes, curIDLocSet = getPoolOverview(fileInfos)

	print
	print 'Final metric', m_best
	return fileInfos


def getTransferList(fileInfos_old, fileInfos_new):
	# Calculate transfers
	def getMap(fi):
		result_loc = {}
		result_size = {}
		for (id, loc, size) in fi:
			result_loc.setdefault(id, []).append(loc)
			result_size[id] = size
		return (result_loc, result_size)
	oldMap, oldSizeMap = getMap(fileInfos_old)
	newMap, newSizeMap = getMap(fileInfos_new)
	assert(len(oldMap) == len(newMap))
	transferList = []
	for id in newMap:
		oldLoc = set(oldMap[id])
		newLoc = set(newMap[id])
		sourceLoc = oldLoc.difference(newLoc)
		targetLoc = newLoc.difference(oldLoc)
		for s, t in zip(sourceLoc, targetLoc):
			transferList.append((id, newSizeMap[id], s, t))
	return transferList


def getDistribution(opts):
	fileInfos = getFileInfos(opts)
	print '%d files selected!' % len(fileInfos)
	if len(fileInfos) == 0:
		sys.exit(0)

	fileInfos_old = list(fileInfos)
	if opts.load_transfers:
		transfers_in = json.load(open(opts.load_transfers))
		print 'Loaded', len(transfers_in), 'transfers'
		fileInfos = applyTransfers(fileInfos, transfers_in)

	pools = {}
	for pool in get_pool_infos(dCacheWebHost):
		if 'space' not in pool:
			print 'Pool %s skipped - no space information available!' % pool['name']
			continue
		pools[pool['name']] = pool
	pools = dict(filter(lambda (k, v): k.endswith('D_cms'), pools.items()))
	totalSize = sum(map(lambda (id, loc, size): size, fileInfos))
	goalFiles = {}
	goalSizes = {}
	for pool in pools:
		poolFrac = pools[pool]['space']['total'] / float(sum(map(lambda p: pools[p]['space']['total'], pools)))
		goalFiles[pool] = int(poolFrac * len(fileInfos))
		goalSizes[pool] = int(poolFrac * totalSize)

	fileInfos_new = optimizeFileInfos_annealing(fileInfos, goalFiles, goalSizes)
	transferList = getTransferList(fileInfos_old, fileInfos_new)

	# Output result of optimizer
	oldFiles, oldSizes, dummy = getPoolOverview(fileInfos_old)
	newFiles, newSizes, dummy = getPoolOverview(fileInfos_new)
	for pool in sorted(pools):
		if pool not in oldFiles or pool not in goalFiles or pool not in newFiles or pool not in goalSizes:
			print pool, "  ----- information missing -----"
			continue
		print pool,
		print '%4d' % oldFiles[pool],
		print '%4d' % goalFiles[pool],
		print '%4d' % newFiles[pool],
		print '%6.1f' % (oldSizes[pool] / 1.e9),
		print '%6.1f' % (goalSizes[pool] / 1.e9),
		print '%6.1f' % (newSizes[pool] / 1.e9),
		print
	print len(transferList), 'transfers out of', len(fileInfos_new), 'files',
	print '%.1f TB transfer volume' % (sum(map(lambda (id, size, s, t): size, transferList)) / 1.e12)
	json.dump(transferList, open(opts.store_transfers, 'w'))

getDistribution(opts)
